{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e07a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /home/root2/CCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5883d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import dataloaders\n",
    "import models\n",
    "import math\n",
    "from utils import Logger\n",
    "from trainer import Trainer\n",
    "import torch.nn.functional as F\n",
    "from utils.losses import abCE_loss, CE_loss, consistency_weight, FocalLoss, softmax_helper, get_alpha\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils.metrics import eval_metrics, AverageMeter\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278afb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "train_logger = Logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11bafb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./configs/config.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5200c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADERS\n",
    "config['train_supervised']['n_labeled_examples'] = config['n_labeled_examples']\n",
    "config['train_unsupervised']['n_labeled_examples'] = config['n_labeled_examples']\n",
    "config['train_unsupervised']['use_weak_lables'] = config['use_weak_lables']\n",
    "supervised_loader = dataloaders.VOC(config['train_supervised'])\n",
    "unsupervised_loader = dataloaders.VOC(config['train_unsupervised'])\n",
    "val_loader = dataloaders.VOC(config['val_loader'])\n",
    "iter_per_epoch = len(unsupervised_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes, ndf=64):\n",
    "        super(FCDiscriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
    "        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8840c3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "class CrossEntropy2d(nn.Module):\n",
    "\n",
    "    def __init__(self, reduction='mean', ignore_label=255):\n",
    "        super(CrossEntropy2d, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ignore_label = ignore_label\n",
    "\n",
    "    def forward(self, predict, target, weight=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                predict:(n, c, h, w)\n",
    "                target:(n, h, w)\n",
    "                weight (Tensor, optional): a manual rescaling weight given to each class.\n",
    "                                           If given, has to be a Tensor of size \"nclasses\"\n",
    "        \"\"\"\n",
    "        assert not target.requires_grad\n",
    "        assert predict.dim() == 4\n",
    "        assert target.dim() == 3\n",
    "        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n",
    "        assert predict.size(2) == target.size(1), \"{0} vs {1} \".format(predict.size(2), target.size(1))\n",
    "        assert predict.size(3) == target.size(2), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n",
    "        n, c, h, w = predict.size()\n",
    "        target_mask = (target >= 0) * (target != self.ignore_label)\n",
    "        target = target[target_mask]\n",
    "        if not target.data.dim():\n",
    "            return torch.zeros(1)\n",
    "        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n",
    "        loss = F.cross_entropy(predict, target, weight=weight, reduction=self.reduction)\n",
    "        return loss\n",
    "    \n",
    "class BCEWithLogitsLoss2d(nn.Module):\n",
    "\n",
    "    def __init__(self, reduction='mean', ignore_label=255):\n",
    "        super(BCEWithLogitsLoss2d, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ignore_label = ignore_label\n",
    "\n",
    "    def forward(self, predict, target, weight=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                predict:(n, 1, h, w)\n",
    "                target:(n, 1, h, w)\n",
    "                weight (Tensor, optional): a manual rescaling weight given to each class.\n",
    "                                           If given, has to be a Tensor of size \"nclasses\"\n",
    "        \"\"\"\n",
    "        assert not target.requires_grad\n",
    "        assert predict.dim() == 4\n",
    "        assert target.dim() == 4\n",
    "        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n",
    "        assert predict.size(2) == target.size(2), \"{0} vs {1} \".format(predict.size(2), target.size(2))\n",
    "        assert predict.size(3) == target.size(3), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n",
    "        n, c, h, w = predict.size()\n",
    "        target_mask = (target >= 0) * (target != self.ignore_label)\n",
    "        target = target[target_mask]\n",
    "        \n",
    "        if not target.data.dim():\n",
    "            return torch.zeros(1)\n",
    "        predict = predict[target_mask]\n",
    "        loss = F.binary_cross_entropy_with_logits(predict, target, weight=weight, reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63de1ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    label = label.cpu().numpy()\n",
    "    one_hot = np.zeros((label.shape[0], 21, label.shape[1], label.shape[2]), dtype=label.dtype)\n",
    "    for i in range(21):\n",
    "        one_hot[:,i,...] = (label==i)\n",
    "    #handle ignore labels\n",
    "    return torch.FloatTensor(one_hot).cuda()\n",
    "\n",
    "def make_D_label(label, ignore_mask):\n",
    "    ignore_mask = np.expand_dims(ignore_mask, axis=1)\n",
    "    D_label = np.ones(ignore_mask.shape)*label\n",
    "    D_label[ignore_mask] = 255\n",
    "    D_label = torch.FloatTensor(D_label)\n",
    "\n",
    "    return D_label.cuda()\n",
    "\n",
    "def loss_calc(pred, label):\n",
    "    \"\"\"\n",
    "    This function returns cross entropy loss for semantic segmentation\n",
    "    \"\"\"\n",
    "    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n",
    "    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n",
    "    label = label.long().cuda()\n",
    "    criterion = CrossEntropy2d().cuda()\n",
    "\n",
    "    return criterion(pred, label)\n",
    "\n",
    "def get_seg_metrics(correct, label, inter, union):\n",
    "    pixAcc = 1.0 * correct / (np.spacing(1) + label)\n",
    "    IoU = 1.0 * inter / (np.spacing(1) + union)\n",
    "    mIoU = IoU.mean()\n",
    "    return {\n",
    "            \"Pixel_Accuracy\": np.round(pixAcc, 3),\n",
    "            \"Mean_IoU\": np.round(mIoU, 3),\n",
    "            \"Class_IoU\": dict(zip(range(21), np.round(IoU, 3)))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51547ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.CCT(num_classes=21, conf=config['model'], testing=True)\n",
    "model_D = FCDiscriminator(num_classes=21).cuda()\n",
    "\n",
    "CCT_checkpoint = torch.load('./saved/CCT_wssl/best_model.pth')\n",
    "model = torch.nn.DataParallel(model)\n",
    "try:\n",
    "    model.load_state_dict(CCT_checkpoint['state_dict'], strict=True)\n",
    "except Exception as e:\n",
    "    print(f'Some modules are missing: {e}')\n",
    "    model.load_state_dict(CCT_checkpoint['state_dict'], strict=False)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2.5e-4, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer_D = torch.optim.Adam(model_D.parameters(), lr=1e-4 ,betas=(0.9, 0.99))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "scheduler_D = torch.optim.lr_scheduler.ConstantLR(optimizer_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d098f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = BCEWithLogitsLoss2d()\n",
    "interp = nn.Upsample(size=(320, 320), mode='bilinear')\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "pred_label = 0\n",
    "gt_label = 1\n",
    "\n",
    "lambda_semi_adv = 0.001\n",
    "lambda_adv_pred = 0.01\n",
    "lambda_semi = 0.1\n",
    "mask_T = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6c58b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture output\n",
    "best_val_mIoU = 0.7345655\n",
    "mIoU_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    dataloader = iter(zip(cycle(supervised_loader), unsupervised_loader))\n",
    "    tbar = tqdm(range(len(unsupervised_loader)), ncols=135)\n",
    "    for batch_idx in tbar:\n",
    "        (input_l, target_l), (input_ul, target_ul) = next(dataloader)\n",
    "        input_l, target_l = input_l.cuda(), target_l.cuda()\n",
    "        input_ul, target_ul = input_ul.cuda(), target_ul.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ##########################\n",
    "        # Train G ################\n",
    "        ##########################\n",
    "        \n",
    "        # Train in semi setting\n",
    "        pred = interp(model(input_ul)) # S(X_n): (B,H,W,C)\n",
    "        pred_remain = pred.detach()\n",
    "\n",
    "        D_out = interp(model_D(F.softmax(pred, dim=1))) # D(S(X_n)): (B,H,W,1)\n",
    "        D_out_sigmoid = torch.sigmoid(D_out).data.cpu().numpy().squeeze(axis=1) # D(S(X_n)): (B,H,W,1)\n",
    "\n",
    "        ignore_mask_remain = np.zeros(D_out_sigmoid.shape).astype(bool) # remain all value, no ignore in D_out_sigmoid\n",
    "        \n",
    "        loss_semi_adv = lambda_semi_adv * bce_loss(D_out, make_D_label(gt_label, ignore_mask_remain))\n",
    "        \n",
    "        # produce ignore mask\n",
    "        semi_ignore_mask = (D_out_sigmoid < mask_T)\n",
    "\n",
    "        semi_gt = pred.data.cpu().numpy().argmax(axis=1) # Self-taught\n",
    "        semi_gt[semi_ignore_mask] = 255 \n",
    "\n",
    "        semi_ratio = 1.0 - float(semi_ignore_mask.sum())/semi_ignore_mask.size\n",
    "        # print('semi ratio: {:.4f}'.format(semi_ratio))\n",
    "\n",
    "        if semi_ratio != 0.0:\n",
    "            semi_gt = torch.FloatTensor(semi_gt)\n",
    "\n",
    "            loss_semi = lambda_semi * loss_calc(pred, semi_gt) # Seft- taught\n",
    "            loss_semi += loss_semi_adv \n",
    "            loss_semi.backward()\n",
    "        \n",
    "        # Train with labeled data\n",
    "        ignore_mask = (target_l.cpu().numpy() == 255)\n",
    "        pred = interp(model(input_l)) # pred change value\n",
    "        \n",
    "        loss_seg = loss_calc(pred, target_l) # L_ce - equation (3)\n",
    "        \n",
    "        D_out = interp(model_D(F.softmax(pred, dim=1)))\n",
    "        \n",
    "        loss_adv_pred = bce_loss(D_out, make_D_label(gt_label, ignore_mask)) # L_adv - equation (4)\n",
    "        \n",
    "        loss = loss_seg + lambda_adv_pred * loss_adv_pred # Equation (2) without semi loss, semi loss is computed above\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        ##########################\n",
    "        # Train D ################\n",
    "        ##########################\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "            \n",
    "        pred = pred.detach() # No backprop to G\n",
    "        pred = torch.cat((pred, pred_remain), 0) # pred - label data, pred_remain: unlabel data\n",
    "        ignore_mask = np.concatenate((ignore_mask,ignore_mask_remain), axis = 0)\n",
    "        D_out = interp(model_D(F.softmax(pred, dim=1)))\n",
    "        loss_D = bce_loss(D_out, make_D_label(pred_label, ignore_mask)) # Part 1 Equation (1)\n",
    "        \n",
    "        D_gt_v = one_hot(target_l).cuda()\n",
    "        ignore_mask_gt = (target_l.cpu().numpy() == 255)\n",
    "        D_out = interp(model_D(D_gt_v))\n",
    "        \n",
    "        loss_D += bce_loss(D_out, make_D_label(gt_label, ignore_mask_gt)) # Part 2 Equation (1)\n",
    "        loss_D.backward()\n",
    "                          \n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Eval mIoU\n",
    "#         seg_metrics_l = eval_metrics(pred[:10], target_l, 21, 255)\n",
    "#         pixel_acc_l, mIoU_l, class_iou_l = get_seg_metrics(*seg_metrics_l).values()\n",
    "#         seg_metrics_u = eval_metrics(pred_remain, target_ul, 21, 255)\n",
    "#         pixel_acc_U, mIoU_u, class_iou_u = get_seg_metrics(*seg_metrics_u).values()\n",
    "        \n",
    "#         tbar.set_description('loss_seg = {0:.3f}, loss_adv_p = {1:.3f}, loss_D = {2:.3f}, loss_semi = {3:.3f}, loss_semi_adv = {4:.3f}, m_l = {5:.3f}, m_u = {6:.3f}'.format(loss_seg.item(), loss_adv_pred.item(), loss_D.item(), loss_semi.item(), loss_semi_adv.item(), mIoU_l, mIoU_u))\n",
    "        tbar.set_description('loss_seg = {0:.3f}, loss_adv_p = {1:.3f}, loss_D = {2:.3f}, loss_semi = {3:.3f}, loss_semi_adv = {4:.3f}'.format(loss_seg.item(), loss_adv_pred.item(), loss_D.item(), loss_semi.item(), loss_semi_adv.item()))\n",
    "#         tbar.set_description('loss_seg = {0:.3f}, loss_adv_p = {1:.3f}, loss_D = {2:.3f}'.format(loss_seg.item(), loss_adv_pred.item(), loss_D.item()))\n",
    "    \n",
    "    scheduler.step()\n",
    "    scheduler_D.step()\n",
    "    \n",
    "    # Eval epoch\n",
    "    model.eval()\n",
    "    total_loss_val = AverageMeter()\n",
    "    total_inter, total_union = 0, 0\n",
    "    total_correct, total_label = 0, 0\n",
    "    mIoU= None\n",
    "    \n",
    "    tbar = tqdm(val_loader, ncols=130)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tbar):\n",
    "            target, data = target.cuda(), data.cuda()\n",
    "\n",
    "            H, W = target.size(1), target.size(2)\n",
    "            up_sizes = (ceil(H / 8) * 8, ceil(W / 8) * 8)\n",
    "            pad_h, pad_w = up_sizes[0] - data.size(2), up_sizes[1] - data.size(3)\n",
    "            data = F.pad(data, pad=(0, pad_w, 0, pad_h), mode='reflect')\n",
    "            output = model(data)\n",
    "            output = output[:, :, :H, :W]\n",
    "            \n",
    "            loss = F.cross_entropy(output, target, ignore_index=255)\n",
    "            total_loss_val.update(loss.item())\n",
    "            correct, labeled, inter, union = eval_metrics(output, target, 21, 255)\n",
    "            total_inter, total_union = total_inter+inter, total_union+union\n",
    "            total_correct, total_label = total_correct+correct, total_label+labeled\n",
    "            \n",
    "            # PRINT INFO\n",
    "            pixAcc = 1.0 * total_correct / (np.spacing(1) + total_label)\n",
    "            IoU = 1.0 * total_inter / (np.spacing(1) + total_union)\n",
    "            mIoU = IoU.mean()\n",
    "            seg_metrics = {\"Pixel_Accuracy\": np.round(pixAcc, 3), \"Mean_IoU\": np.round(mIoU, 3),\n",
    "                            \"Class_IoU\": dict(zip(range(21), np.round(IoU, 3)))}\n",
    "\n",
    "            tbar.set_description('EVAL ({}) | Loss: {:.3f}, PixelAcc: {:.2f}, Mean IoU: {:.3f} |'.format( epoch,\n",
    "                                            total_loss_val.average, pixAcc, mIoU))\n",
    "        \n",
    "    mIoU_hist.append(mIoU)    \n",
    "    if mIoU > best_val_mIoU:\n",
    "        best_val_mIoU = mIoU\n",
    "        state = {\n",
    "            'CCT_state_dict': model.state_dict(),\n",
    "            'discriminator_state_dict': model_D.state_dict()\n",
    "        }\n",
    "        os.makedirs(f'./saved/GAN/{best_val_mIoU}')\n",
    "        torch.save(state, f'./saved/GAN/{best_val_mIoU}/best_model.pth')\n",
    "        print('Save best checkpoint')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c272bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_val_mIoU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
