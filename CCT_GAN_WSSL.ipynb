{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44291158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/root2/CCT\n"
     ]
    }
   ],
   "source": [
    "# %cd /home/root2/CCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83e7e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import torch\n",
    "import dataloaders\n",
    "import models\n",
    "import math\n",
    "from utils import Logger\n",
    "from trainer import Trainer\n",
    "import torch.nn.functional as F\n",
    "from utils.losses import abCE_loss, CE_loss, consistency_weight, FocalLoss, softmax_helper, get_alpha\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from utils.metrics import eval_metrics, AverageMeter\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a531fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f807387ce30>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8331707",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = json.load(open('./configs/config_wssl.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9341ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA LOADERS\n",
    "config['train_supervised']['n_labeled_examples'] = config['n_labeled_examples']\n",
    "config['train_unsupervised']['n_labeled_examples'] = config['n_labeled_examples']\n",
    "config['train_unsupervised']['use_weak_lables'] = config['use_weak_lables']\n",
    "supervised_loader = dataloaders.VOC(config['train_supervised'])\n",
    "unsupervised_loader = dataloaders.VOC(config['train_unsupervised'])\n",
    "val_loader = dataloaders.VOC(config['val_loader'])\n",
    "iter_per_epoch = len(unsupervised_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1919bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FCDiscriminator(nn.Module):\n",
    "    def __init__(self, num_classes, ndf=64):\n",
    "        super(FCDiscriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(num_classes, ndf, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf*8, kernel_size=4, stride=2, padding=1)\n",
    "        self.classifier = nn.Conv2d(ndf*8, 1, kernel_size=4, stride=2, padding=1)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.leaky_relu(self.conv2(x))\n",
    "        x = self.leaky_relu(self.conv3(x))\n",
    "        x = self.leaky_relu(self.conv4(x))\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fad7d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "class CrossEntropy2d(nn.Module):\n",
    "\n",
    "    def __init__(self, reduction='mean', ignore_label=255):\n",
    "        super(CrossEntropy2d, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ignore_label = ignore_label\n",
    "\n",
    "    def forward(self, predict, target, weight=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                predict:(n, c, h, w)\n",
    "                target:(n, h, w)\n",
    "                weight (Tensor, optional): a manual rescaling weight given to each class.\n",
    "                                           If given, has to be a Tensor of size \"nclasses\"\n",
    "        \"\"\"\n",
    "        assert not target.requires_grad\n",
    "        assert predict.dim() == 4\n",
    "        assert target.dim() == 3\n",
    "        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n",
    "        assert predict.size(2) == target.size(1), \"{0} vs {1} \".format(predict.size(2), target.size(1))\n",
    "        assert predict.size(3) == target.size(2), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n",
    "        n, c, h, w = predict.size()\n",
    "        target_mask = (target >= 0) * (target != self.ignore_label)\n",
    "        target = target[target_mask]\n",
    "        if not target.data.dim():\n",
    "            return torch.zeros(1)\n",
    "        predict = predict.transpose(1, 2).transpose(2, 3).contiguous()\n",
    "        predict = predict[target_mask.view(n, h, w, 1).repeat(1, 1, 1, c)].view(-1, c)\n",
    "        loss = F.cross_entropy(predict, target, weight=weight, reduction=self.reduction)\n",
    "        return loss\n",
    "    \n",
    "class BCEWithLogitsLoss2d(nn.Module):\n",
    "\n",
    "    def __init__(self, reduction='mean', ignore_label=255):\n",
    "        super(BCEWithLogitsLoss2d, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.ignore_label = ignore_label\n",
    "\n",
    "    def forward(self, predict, target, weight=None):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                predict:(n, 1, h, w)\n",
    "                target:(n, 1, h, w)\n",
    "                weight (Tensor, optional): a manual rescaling weight given to each class.\n",
    "                                           If given, has to be a Tensor of size \"nclasses\"\n",
    "        \"\"\"\n",
    "        assert not target.requires_grad\n",
    "        assert predict.dim() == 4\n",
    "        assert target.dim() == 4\n",
    "        assert predict.size(0) == target.size(0), \"{0} vs {1} \".format(predict.size(0), target.size(0))\n",
    "        assert predict.size(2) == target.size(2), \"{0} vs {1} \".format(predict.size(2), target.size(2))\n",
    "        assert predict.size(3) == target.size(3), \"{0} vs {1} \".format(predict.size(3), target.size(3))\n",
    "        n, c, h, w = predict.size()\n",
    "        target_mask = (target >= 0) * (target != self.ignore_label)\n",
    "        target = target[target_mask]\n",
    "        \n",
    "        if not target.data.dim():\n",
    "            return torch.zeros(1)\n",
    "        predict = predict[target_mask]\n",
    "        loss = F.binary_cross_entropy_with_logits(predict, target, weight=weight, reduction=self.reduction)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be70d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(label):\n",
    "    label = label.cpu().numpy()\n",
    "    one_hot = np.zeros((label.shape[0], 21, label.shape[1], label.shape[2]), dtype=label.dtype)\n",
    "    for i in range(21):\n",
    "        one_hot[:,i,...] = (label==i)\n",
    "    #handle ignore labels\n",
    "    return torch.FloatTensor(one_hot).cuda()\n",
    "\n",
    "def make_D_label(label, ignore_mask):\n",
    "    ignore_mask = np.expand_dims(ignore_mask, axis=1)\n",
    "    D_label = np.ones(ignore_mask.shape)*label\n",
    "    D_label[ignore_mask] = 255\n",
    "    D_label = torch.FloatTensor(D_label)\n",
    "\n",
    "    return D_label.cuda()\n",
    "\n",
    "def loss_calc(pred, label):\n",
    "    \"\"\"\n",
    "    This function returns cross entropy loss for semantic segmentation\n",
    "    \"\"\"\n",
    "    # out shape batch_size x channels x h x w -> batch_size x channels x h x w\n",
    "    # label shape h x w x 1 x batch_size  -> batch_size x 1 x h x w\n",
    "    label = label.long().cuda()\n",
    "    criterion = CrossEntropy2d().cuda()\n",
    "\n",
    "    return criterion(pred, label)\n",
    "\n",
    "def get_seg_metrics(correct, label, inter, union):\n",
    "    pixAcc = 1.0 * correct / (np.spacing(1) + label)\n",
    "    IoU = 1.0 * inter / (np.spacing(1) + union)\n",
    "    mIoU = IoU.mean()\n",
    "    return {\n",
    "            \"Pixel_Accuracy\": np.round(pixAcc, 3),\n",
    "            \"Mean_IoU\": np.round(mIoU, 3),\n",
    "            \"Class_IoU\": dict(zip(range(21), np.round(IoU, 3)))\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d22c2b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pretrained model:models/backbones/pretrained/3x3resnet50-imagenet.pth\n"
     ]
    }
   ],
   "source": [
    "model = models.CCT(num_classes=21, conf=config['model'], testing=True)\n",
    "model_D = FCDiscriminator(num_classes=21).cuda()\n",
    "\n",
    "CCT_checkpoint = torch.load('./saved/CCT_wssl/best_model.pth')\n",
    "model = torch.nn.DataParallel(model)\n",
    "try:\n",
    "    model.load_state_dict(CCT_checkpoint['state_dict'], strict=True)\n",
    "except Exception as e:\n",
    "    print(f'Some modules are missing: {e}')\n",
    "    model.load_state_dict(CCT_checkpoint['state_dict'], strict=False)\n",
    "model.cuda()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=2.5e-4, momentum=0.9, weight_decay=1e-4)\n",
    "optimizer_D = torch.optim.Adam(model_D.parameters(), lr=1e-4 ,betas=(0.9, 0.99))\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ConstantLR(optimizer)\n",
    "scheduler_D = torch.optim.lr_scheduler.ConstantLR(optimizer_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c7d9e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = BCEWithLogitsLoss2d()\n",
    "interp = nn.Upsample(size=(320, 320), mode='bilinear')\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "pred_label = 0\n",
    "gt_label = 1\n",
    "\n",
    "lambda_semi_adv = 0.001\n",
    "lambda_adv_pred = 0.01\n",
    "lambda_semi = 0.1\n",
    "mask_T = 0.2\n",
    "weakly_loss_w = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1dd09306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_seg = 0.050, loss_adv_p = 2.269, loss_D = 0.657, loss_semi = 0.004, loss_semi_adv = 0.003, loss_weakly = 0.007: 100%|█| 912/912 [3\n",
      "EVAL (0) | Loss: 0.298, PixelAcc: 0.94, Mean IoU: 0.732 |: 100%|██████████████████████████████| 1449/1449 [01:12<00:00, 19.99it/s]\n",
      "loss_seg = 0.020, loss_adv_p = 1.263, loss_D = 0.821, loss_semi = 0.003, loss_semi_adv = 0.003, loss_weakly = 0.003: 100%|█| 912/912 [3\n",
      "EVAL (1) | Loss: 0.305, PixelAcc: 0.94, Mean IoU: 0.725 |: 100%|██████████████████████████████| 1449/1449 [01:11<00:00, 20.25it/s]\n",
      "loss_seg = 0.039, loss_adv_p = 2.321, loss_D = 0.639, loss_semi = 0.004, loss_semi_adv = 0.004, loss_weakly = 0.005: 100%|█| 912/912 [3\n",
      "EVAL (2) | Loss: 0.293, PixelAcc: 0.94, Mean IoU: 0.732 |: 100%|██████████████████████████████| 1449/1449 [01:10<00:00, 20.61it/s]\n",
      "loss_seg = 0.025, loss_adv_p = 3.173, loss_D = 0.930, loss_semi = 0.005, loss_semi_adv = 0.005, loss_weakly = 0.003: 100%|█| 912/912 [3\n",
      "EVAL (3) | Loss: 0.290, PixelAcc: 0.94, Mean IoU: 0.735 |: 100%|██████████████████████████████| 1449/1449 [01:11<00:00, 20.14it/s]\n",
      "loss_seg = 0.039, loss_adv_p = 2.933, loss_D = 0.558, loss_semi = 0.003, loss_semi_adv = 0.003, loss_weakly = 0.003: 100%|█| 912/912 [3\n",
      "EVAL (4) | Loss: 0.304, PixelAcc: 0.94, Mean IoU: 0.729 |: 100%|██████████████████████████████| 1449/1449 [01:12<00:00, 20.09it/s]\n",
      "loss_seg = 0.016, loss_adv_p = 1.392, loss_D = 0.803, loss_semi = 0.002, loss_semi_adv = 0.002, loss_weakly = 0.003:  74%|▋| 672/912 [2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m D_out \u001b[38;5;241m=\u001b[39m interp(model_D(F\u001b[38;5;241m.\u001b[39msoftmax(pred, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     77\u001b[0m loss_D \u001b[38;5;241m=\u001b[39m bce_loss(D_out, make_D_label(pred_label, ignore_mask)) \u001b[38;5;66;03m# Part 1 Equation (1)\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m D_gt_v \u001b[38;5;241m=\u001b[39m \u001b[43mone_hot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_l\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     80\u001b[0m ignore_mask_gt \u001b[38;5;241m=\u001b[39m (target_l\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m255\u001b[39m)\n\u001b[1;32m     81\u001b[0m D_out \u001b[38;5;241m=\u001b[39m interp(model_D(D_gt_v))\n",
      "Cell \u001b[0;32mIn [8], line 5\u001b[0m, in \u001b[0;36mone_hot\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m      3\u001b[0m one_hot \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m21\u001b[39m, label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], label\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), dtype\u001b[38;5;241m=\u001b[39mlabel\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m21\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     one_hot[:,i,\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m (label\u001b[38;5;241m==\u001b[39mi)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#handle ignore labels\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(one_hot)\u001b[38;5;241m.\u001b[39mcuda()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %%capture output\n",
    "best_val_mIoU = 0.74\n",
    "mIoU_hist = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    dataloader = iter(zip(cycle(supervised_loader), unsupervised_loader))\n",
    "    tbar = tqdm(range(len(unsupervised_loader)), ncols=135)\n",
    "    for batch_idx in tbar:\n",
    "        (input_l, target_l), (input_ul, target_ul) = next(dataloader)\n",
    "        input_l, target_l = input_l.cuda(), target_l.cuda()\n",
    "        input_ul, target_ul = input_ul.cuda(), target_ul.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ##########################\n",
    "        # Train G ################\n",
    "        ##########################\n",
    "        \n",
    "        # Train in semi setting\n",
    "        pred = interp(model(input_ul)) # S(X_n): (B,C,H,W)\n",
    "\n",
    "        D_out = interp(model_D(F.softmax(pred, dim=1))) # D(S(X_n)): (B,1,H,W)\n",
    "        D_out_sigmoid = torch.sigmoid(D_out).data.cpu().numpy().squeeze(axis=1) # D(S(X_n)): (B,1,H,W)\n",
    "\n",
    "        ignore_mask_remain = np.zeros(D_out_sigmoid.shape).astype(bool) # remain all value, no ignore in D_out_sigmoid\n",
    "        \n",
    "        loss_semi_adv = lambda_semi_adv * bce_loss(D_out, make_D_label(gt_label, ignore_mask_remain))\n",
    "        \n",
    "        # produce ignore mask\n",
    "        semi_ignore_mask = (D_out_sigmoid < mask_T)\n",
    "\n",
    "        semi_gt = pred.data.cpu().numpy().argmax(axis=1) # Pred with high confidence is self-taught label\n",
    "        semi_gt[semi_ignore_mask] = 255 \n",
    "\n",
    "        semi_ratio = 1.0 - float(semi_ignore_mask.sum())/semi_ignore_mask.size\n",
    "        # print('semi ratio: {:.4f}'.format(semi_ratio))\n",
    "\n",
    "        semi_gt = torch.FloatTensor(semi_gt)\n",
    "\n",
    "        loss_semi = lambda_semi * loss_calc(pred, semi_gt) # Seft-taught\n",
    "        loss_semi += loss_semi_adv \n",
    "#             loss_semi.backward()\n",
    "        \n",
    "        # Train with weak label\n",
    "        loss_weakly = weakly_loss_w * loss_calc(pred, target_ul)\n",
    "#         loss_weakly.backward()\n",
    "        (loss_semi + loss_weakly).backward()\n",
    "        \n",
    "        pred_remain = pred.detach()\n",
    "        \n",
    "        # Train with labeled data\n",
    "        ignore_mask = (target_l.cpu().numpy() == 255)\n",
    "        pred = interp(model(input_l)) # pred change value\n",
    "        \n",
    "        loss_seg = loss_calc(pred, target_l) # L_ce - equation (3)\n",
    "        \n",
    "        D_out = interp(model_D(F.softmax(pred, dim=1)))\n",
    "        \n",
    "        loss_adv_pred = bce_loss(D_out, make_D_label(gt_label, ignore_mask)) # L_adv - equation (4)\n",
    "        \n",
    "        loss = loss_seg + lambda_adv_pred * loss_adv_pred # Equation (2) without semi loss, semi loss is computed above\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        ##########################\n",
    "        # Train D ################\n",
    "        ##########################\n",
    "        \n",
    "        optimizer_D.zero_grad()\n",
    "            \n",
    "        pred = pred.detach() # No backprop to G\n",
    "        pred = torch.cat((pred, pred_remain), 0) # pred - label data, pred_remain: unlabel data\n",
    "        ignore_mask = np.concatenate((ignore_mask,ignore_mask_remain), axis = 0)\n",
    "        D_out = interp(model_D(F.softmax(pred, dim=1)))\n",
    "        loss_D = bce_loss(D_out, make_D_label(pred_label, ignore_mask)) # Part 1 Equation (1)\n",
    "        \n",
    "        D_gt_v = one_hot(target_l).cuda()\n",
    "        ignore_mask_gt = (target_l.cpu().numpy() == 255)\n",
    "        D_out = interp(model_D(D_gt_v))\n",
    "        \n",
    "        loss_D += bce_loss(D_out, make_D_label(gt_label, ignore_mask_gt)) # Part 2 Equation (1)\n",
    "        loss_D.backward()\n",
    "                          \n",
    "        optimizer_D.step()\n",
    "        \n",
    "        \n",
    "        tbar.set_description('loss_seg = {0:.3f}, loss_adv_p = {1:.3f}, loss_D = {2:.3f}, loss_semi = {3:.3f}, loss_semi_adv = {4:.3f}, loss_weakly = {5:.3f}'.format(loss_seg.item(), loss_adv_pred.item(), loss_D.item(), loss_semi.item(), loss_semi_adv.item(), loss_weakly.item()))\n",
    "    \n",
    "    scheduler.step()\n",
    "    scheduler_D.step()\n",
    "    \n",
    "    # Eval epoch\n",
    "    model.eval()\n",
    "    total_loss_val = AverageMeter()\n",
    "    total_inter, total_union = 0, 0\n",
    "    total_correct, total_label = 0, 0\n",
    "    mIoU= None\n",
    "    \n",
    "    tbar = tqdm(val_loader, ncols=130)\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(tbar):\n",
    "            target, data = target.cuda(), data.cuda()\n",
    "\n",
    "            H, W = target.size(1), target.size(2)\n",
    "            up_sizes = (ceil(H / 8) * 8, ceil(W / 8) * 8)\n",
    "            pad_h, pad_w = up_sizes[0] - data.size(2), up_sizes[1] - data.size(3)\n",
    "            data = F.pad(data, pad=(0, pad_w, 0, pad_h), mode='reflect')\n",
    "            output = model(data)\n",
    "            output = output[:, :, :H, :W]\n",
    "            \n",
    "            loss = F.cross_entropy(output, target, ignore_index=255)\n",
    "            total_loss_val.update(loss.item())\n",
    "            correct, labeled, inter, union = eval_metrics(output, target, 21, 255)\n",
    "            total_inter, total_union = total_inter+inter, total_union+union\n",
    "            total_correct, total_label = total_correct+correct, total_label+labeled\n",
    "            \n",
    "            # PRINT INFO\n",
    "            pixAcc = 1.0 * total_correct / (np.spacing(1) + total_label)\n",
    "            IoU = 1.0 * total_inter / (np.spacing(1) + total_union)\n",
    "            mIoU = IoU.mean()\n",
    "            seg_metrics = {\"Pixel_Accuracy\": np.round(pixAcc, 3), \"Mean_IoU\": np.round(mIoU, 3),\n",
    "                            \"Class_IoU\": dict(zip(range(21), np.round(IoU, 3)))}\n",
    "\n",
    "            tbar.set_description('EVAL ({}) | Loss: {:.3f}, PixelAcc: {:.2f}, Mean IoU: {:.3f} |'.format( epoch,\n",
    "                                            total_loss_val.average, pixAcc, mIoU))\n",
    "        \n",
    "    mIoU_hist.append(mIoU)    \n",
    "    if mIoU > best_val_mIoU:\n",
    "        best_val_mIoU = mIoU\n",
    "        state = {\n",
    "            'CCT_state_dict': model.state_dict(),\n",
    "            'discriminator_state_dict': model_D.state_dict()\n",
    "        }\n",
    "        os.makedirs(f'./saved/GAN_WSSL/{best_val_mIoU}')\n",
    "        torch.save(state, f'./saved/GAN_WSSL/{best_val_mIoU}/best_model.pth')\n",
    "        print('Save best checkpoint')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best validation mIoU: ', best_val_mIoU)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
